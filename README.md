# Titanic Survival Prediction ML Pipeline

An end-to-end machine learning pipeline for predicting Titanic passenger survival, featuring hyperparameter optimization with Optuna and experiment tracking with MLflow.

## ğŸ“Œ Features

- **Data Processing**: Automated cleaning and feature engineering
- **Model Training**: 
  - Random Forest Classifier
  - Gradient Boosting Classifier
- **Hyperparameter Optimization**: Automated with Optuna
- **Experiment Tracking**: Full MLflow integration
- **Reproducible Pipeline**: Airflow DAG for workflow orchestration
- **Modular Design**: Clean separation of components

## ğŸ› ï¸ Project Structure
```
mlops_project/
â”œâ”€â”€ src/                        # Main source code
â”‚ â”œâ”€â”€ __init__.py
â”‚ â”œâ”€â”€ data/                     # Data block
â”‚ â”‚ â”œâ”€â”€ __init__.py  
â”‚ â”‚ â”œâ”€â”€ loader.py               # Data loading utilities
â”‚ â”‚ â””â”€â”€ preprocessor.py         # Feature engineering
â”‚ â”œâ”€â”€ models/                   # ML components
â”‚ â”‚ â”œâ”€â”€ __init__.py  
â”‚ â”‚ â”œâ”€â”€ model_factory.py        # Model creation
â”‚ â”‚ â”œâ”€â”€ model.py                # Model implementations
â”‚ â”‚ â””â”€â”€ optuna_optimizer.py     # Hyperparameter tuning
â”‚ â””â”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ __init__.py  
â”‚ â”‚ â””â”€â”€ config.py               # Path configuration
â”‚ â””â”€â”€ visualization/            # Visualization block (in development)
â”‚ â”‚ â”œâ”€â”€ __init__.py  
â”‚ â”‚ â””â”€â”€ plotter.py              # For plotting (example barblot of feature's importance)
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ titanic_pipeline.py       # Airflow DAG definition
â”œâ”€â”€ data/                       # Input data
â”‚ â”‚â”€â”€ gender_submission.csv     # Sample submission
â”‚ â”‚â”€â”€ test.csv                  # Test data
â”‚ â””â”€â”€ train.csv                 # Train data
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ exploring_data.ipynb      
â”œâ”€â”€ scripts/                    # Utility scripts (in development)
â”‚ â”‚â”€â”€ test.py                   
â”‚ â””â”€â”€ train.py                 
â”œâ”€â”€ submissions/                # Prediction outputs
â”œâ”€â”€ mlruns/                     # MLflow tracking data (hidden)
â”œâ”€â”€ airflow/                    # Airflow home directory (hidden)
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Edit `src/utils/config.py` to configure:

```python
# Data paths
DATA_DIR = BASE_DIR / "data"
TRAIN_DATA = DATA_DIR / "train.csv" 

# MLflow tracking
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000" 
MLFLOW_EXPERIMENT_NAME = "titanic_survival"
```

## Installation
1. Clone the repository:
```bash
git clone https://github.com/yourusername/titanic-ml-pipeline.git
cd titanic-ml-pipeline
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

3. Set environment variables:
```bash
export PYTHONPATH="${PYTHONPATH}:{path_to_project}"    
export AIRFLOW_HOME=$(pwd)/airflow
```

## Running pipeline

1. Initialize Airflow
```bash
airflow standalone
```
2. Start MLflow tracking server
```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000
```

3. Access the Airflow web interface (typically at http://localhost:8080) and trigger the titanic_pipeline DAG.
